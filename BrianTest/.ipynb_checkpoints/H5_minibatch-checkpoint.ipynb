{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "import torch \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from file_io import read, write\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 root,\n",
    "                 input_tfm_function,\n",
    "                 gt_tfm_function,\n",
    "                 input_folder_1 = \"image_2\", \n",
    "                 input_folder_2 = \"image_2\", \n",
    "                 gt_folder = \"flow_occ\",\n",
    "                 input_search_param_1=\"*_10.png\", \n",
    "                 input_search_param_2 = \"*_11.png\", \n",
    "                 gt_search_param = \"*_10.png\"):\n",
    "        \n",
    "        self.root = Path(root)\n",
    "        self.input1_dir = self.root/input_folder_1\n",
    "        self.input2_dir = self.root/input_folder_2\n",
    "        self.gt_dir = self.root/gt_folder\n",
    "        \n",
    "        self.input_1_names = sorted([os.path.basename(x) for x in self.input1_dir.glob(input_search_param_1)])\n",
    "        self.input_2_names = sorted([os.path.basename(x) for x in self.input2_dir.glob(input_search_param_2)])\n",
    "        self.gt_names = sorted([os.path.basename(x) for x in self.gt_dir.glob(gt_search_param)])\n",
    "        self.input_tfm_function = input_tfm_function\n",
    "        self.gt_tfm_function = gt_tfm_function\n",
    "        \n",
    "#         for a,b,c in zip(self.input_1_names,self.input_2_names,self.gt_names):\n",
    "#             print(a,b,c)\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_1_names)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "#         print(idx)\n",
    "        print(self.input_1_names[idx],self.input_2_names[idx],self.gt_names[idx])\n",
    "        input1 = read(str(self.input1_dir/self.input_1_names[idx]))\n",
    "        input2 = read(str(self.input2_dir/self.input_2_names[idx]))\n",
    "        gt = read(str(self.gt_dir/self.gt_names[idx]))\n",
    "        \n",
    "        input1 = self.input_tfm_function(input1)\n",
    "        input2 = self.input_tfm_function(input2)\n",
    "        gt = self.gt_tfm_function(gt)\n",
    "        \n",
    "        return input1, input2, gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the transform\n",
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((512,512), interpolation=2),\n",
    "        transforms.ToTensor()\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader_kitti = dataset(\"../Data/KittiDataset/training\",\n",
    "#                        transform,transform,\n",
    "#                        input_folder_1 = \"image_2\", \n",
    "#                        input_folder_2 = \"image_2\", \n",
    "#                        gt_folder = \"flow_occ\",\n",
    "#                        input_search_param_1=\"*_10.png\", \n",
    "#                        input_search_param_2 = \"*_11.png\", \n",
    "#                        gt_search_param = \"*_10.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_FlyingChairs = dataset(\"../Data/ChairsSDHom_extended\",\n",
    "                       transform,transform,\n",
    "                       input_folder_1 = \"train\", \n",
    "                       input_folder_2 = \"train\", \n",
    "                       gt_folder = \"train\",\n",
    "                       input_search_param_1=\"*-img_0.png\", \n",
    "                       input_search_param_2 = \"*-img_1.png\", \n",
    "                       gt_search_param = \"*-flow_01.flo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_kitti = torch.utils.data.DataLoader(train_loader_kitti,10,shuffle=True,num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_flyingChairs = torch.utils.data.DataLoader(train_loader_FlyingChairs,10,shuffle=True,num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_H5(dataset, path):\n",
    "    index = 0\n",
    "    for inputs1,inputs2,target in dataset:\n",
    "        print(index)\n",
    "        with h5py.File(path + str(index), 'w') as f:\n",
    "            in1 = f.create_dataset('img1', dtype=np.float32,data=inputs1,compression=\"gzip\", compression_opts=9)\n",
    "            in2 = f.create_dataset('img2', dtype=np.float32,data=inputs2,compression=\"gzip\", compression_opts=9)    \n",
    "            targ = f.create_dataset('target', dtype=np.float32,data=target,compression=\"gzip\", compression_opts=9)\n",
    "            index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_H5(index, path):\n",
    "    with h5py.File(path + str(index), 'r') as f:\n",
    "        img1 = f[\"img1\"][()]\n",
    "        img2 = f[\"img2\"][()]\n",
    "        trg = f[\"target\"][()]\n",
    "        img1_file = torch.from_numpy(img1)\n",
    "        img2_file = torch.from_numpy(img2)\n",
    "        target_file = torch.from_numpy(trg)\n",
    "        return img1_file,img2_file,target_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_batch(im1,im2,target):\n",
    "    for image_1, image_2, target_image in zip(im1,im2,target):\n",
    "        im1_print = image_1.numpy().transpose((1,2,0))\n",
    "        im2_print = image_2.numpy().transpose((1,2,0))\n",
    "        \n",
    "        print(target_image.shape)\n",
    "\n",
    "        z = np.zeros((target_image.shape[1],target_image.shape[2]))\n",
    "        target_print = np.stack((target_image[0],target_image[1], z), axis=0)\n",
    "        target_print = target_print.transpose((1,2,0))\n",
    "        print(target_print.shape)\n",
    "\n",
    "        fig, ax = plt.subplots(1, 3, figsize=(15, 20))\n",
    "        for a in ax:\n",
    "          a.set_axis_off()\n",
    "\n",
    "        ax[0].imshow(im1_print)\n",
    "        ax[1].imshow(im2_print)\n",
    "        ax[2].imshow(target_print)\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_H5(dataset_kitti, path = '../Data/H5/512by512Kitti/mini_batch_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Caught Exception in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/Users/brianandika/opt/anaconda3/envs/pytorch-nn/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/Users/brianandika/opt/anaconda3/envs/pytorch-nn/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/Users/brianandika/opt/anaconda3/envs/pytorch-nn/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-2-44ea0e88117f>\", line 36, in __getitem__\n    gt = read(str(self.gt_dir/self.gt_names[idx]))\n  File \"/Users/brianandika/LocalFolders/EECS504FinalProject/EECS-504-PWC-Net/BrianTest/file_io.py\", line 13, in read\n    elif file.endswith('.flo'): return readFlow(file)\n  File \"/Users/brianandika/LocalFolders/EECS504FinalProject/EECS-504-PWC-Net/BrianTest/file_io.py\", line 105, in readFlow\n    raise Exception('Flow file header does not contain PIEH')\nException: Flow file header does not contain PIEH\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-f1f0a1d494b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwrite_H5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_flyingChairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../Data/H5/512by512FlyingChairs/mini_batch_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-4072cf9366cf>\u001b[0m in \u001b[0;36mwrite_H5\u001b[0;34m(dataset, path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwrite_H5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0minputs1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minputs2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pytorch-nn/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pytorch-nn/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    854\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pytorch-nn/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 881\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    882\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pytorch-nn/lib/python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mException\u001b[0m: Caught Exception in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/Users/brianandika/opt/anaconda3/envs/pytorch-nn/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/Users/brianandika/opt/anaconda3/envs/pytorch-nn/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/Users/brianandika/opt/anaconda3/envs/pytorch-nn/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-2-44ea0e88117f>\", line 36, in __getitem__\n    gt = read(str(self.gt_dir/self.gt_names[idx]))\n  File \"/Users/brianandika/LocalFolders/EECS504FinalProject/EECS-504-PWC-Net/BrianTest/file_io.py\", line 13, in read\n    elif file.endswith('.flo'): return readFlow(file)\n  File \"/Users/brianandika/LocalFolders/EECS504FinalProject/EECS-504-PWC-Net/BrianTest/file_io.py\", line 105, in readFlow\n    raise Exception('Flow file header does not contain PIEH')\nException: Flow file header does not contain PIEH\n"
     ]
    }
   ],
   "source": [
    "write_H5(dataset_flyingChairs, path = '../Data/H5/512by512FlyingChairs/mini_batch_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im1,im2,target = read_H5(2,path = '../Data/H5/512by512FlyingChairs/mini_batch_')\n",
    "print_batch(im1,im2,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
