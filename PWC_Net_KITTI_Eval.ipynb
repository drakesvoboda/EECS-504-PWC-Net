{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PWC-Net_KITTI_Eval.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drakesvoboda/EECS-504-PWC-Net/blob/master/PWC_Net_KITTI_Eval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKzvzdkiUcQo",
        "colab_type": "text"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUxSlIg6Ukl2",
        "colab_type": "text"
      },
      "source": [
        "## Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqEExLxQw2dR",
        "colab_type": "code",
        "outputId": "9e1795c8-8ade-4a4a-9d6a-92d05682af43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EbM-yjTUzpa",
        "colab_type": "text"
      },
      "source": [
        "## Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAPzrZttjidB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9QUbU8jYAAr",
        "colab_type": "code",
        "outputId": "01708f3b-19ff-4254-9a01-a9f60c54fc28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "!pip install pypng\n",
        "!pip install flow_vis\n",
        "!pip install spatial-correlation-sampler\n",
        "#!git clone https://github.com/NVlabs/PWC-Net.git\n",
        "#!git clone https://github.com/NVIDIA/flownet2-pytorch.git\n",
        "!git clone https://github.com/drakesvoboda/EECS-504-PWC-Net"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pypng in /usr/local/lib/python3.6/dist-packages (0.0.20)\n",
            "Requirement already satisfied: flow_vis in /usr/local/lib/python3.6/dist-packages (0.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from flow_vis) (1.18.2)\n",
            "Requirement already satisfied: spatial-correlation-sampler in /usr/local/lib/python3.6/dist-packages (0.2.1)\n",
            "Requirement already satisfied: torch>=1.1 in /usr/local/lib/python3.6/dist-packages (from spatial-correlation-sampler) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from spatial-correlation-sampler) (1.18.2)\n",
            "fatal: destination path 'EECS-504-PWC-Net' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VS-eMrrpmwYs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sys.path.append('/content/EECS-504-PWC-Net')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xK3v1v6rMV2o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from train import *\n",
        "from util import *\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, Subset, RandomSampler\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "import cv2\n",
        "import png\n",
        "from PIL import Image\n",
        "import flow_vis\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from spatial_correlation_sampler import spatial_correlation_sample\n",
        "\n",
        "#from networks.correlation_package.correlation import Correlation\n",
        "#import correlation_cuda"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjgGIfmqRx74",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Correlation(nn.Module):\n",
        "  def __init__(self, *args, **kwargs):\n",
        "    super(Correlation, self).__init__()\n",
        "\n",
        "  def forward(self, input1, input2):\n",
        "\n",
        "    #     For a correlation with max displacement of 4, the patch size to search is 9 by 9\n",
        "    #     Below is a 9 by 9 patch (with max displacement of 4)\n",
        "    #\n",
        "    #     4   x x x x | x x x x \n",
        "    #     3   x x x x | x x x x \n",
        "    #     2   x x x x | x x x x \n",
        "    #     1   x x x x | x x x x \n",
        "    #     0   - - - - O - - - - \n",
        "    #     1   x x x x | x x x x \n",
        "    #     2   x x x x | x x x x \n",
        "    #     3   x x x x | x x x x \n",
        "    #     4   x x x x | x x x x \n",
        "    #\n",
        "    #         4 3 2 1 0 1 2 3 4\n",
        "\n",
        "    # https://github.com/NVlabs/PWC-Net/issues/60#issuecomment-496055396\n",
        "    # input1 = F.normalize(input1, p=2, dim=2)\n",
        "    # input2 = F.normalize(input2, p=2, dim=2)\n",
        "\n",
        "    out_corr = spatial_correlation_sample(input1,\n",
        "                                          input2,\n",
        "                                          kernel_size=1,\n",
        "                                          patch_size=9,\n",
        "                                          stride=1,\n",
        "                                          padding=0,\n",
        "                                          dilation_patch=1)\n",
        "    \n",
        "\n",
        "    # collate dimensions 1 and 2 in order to be treated as a\n",
        "    # regular 4D tensor\n",
        "    b, ph, pw, h, w = out_corr.size()\n",
        "    out_corr = out_corr.view(b, ph * pw, h, w)/input1.size(1)\n",
        "    return out_corr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5_8OEmzY8Qh",
        "colab_type": "text"
      },
      "source": [
        "# Dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3H3nJ0RiTkbT",
        "colab_type": "text"
      },
      "source": [
        "##Transform"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgAV218DWqqY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resample_mask(mask, size):\n",
        "    \"\"\"\n",
        "    mask: mask map to be resampled\n",
        "    size: new mask map size. Must be [height,weight]\n",
        "    \"\"\"\n",
        "    mask = cv2.resize(mask,dsize=(size[1],size[0]),interpolation=cv2.INTER_NEAREST)\n",
        "    valid_idx = (mask != 0)\n",
        "    mask[valid_idx] = 1\n",
        "    mask = np.expand_dims(mask, axis=2)\n",
        "    return mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OwKmkLkTosO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FlowDatasetTransform():\n",
        "    def __init__(self, norm, size=(375, 1242), crop=(320, 896), flip=0.5):\n",
        "        self.norm = norm\n",
        "        self.size = size\n",
        "        self.crop = crop\n",
        "        self.flip = flip\n",
        "\n",
        "        mean = np.array(self.norm.mean)\n",
        "        std = np.array(self.norm.std)\n",
        "\n",
        "        self.inv_norm = transforms.Normalize(mean=-mean/std, std=1/std)\n",
        "\n",
        "    def __call__(self, im_1, im_2, target):\n",
        "        resize = transforms.Resize(size=self.size)\n",
        "        \n",
        "        im_1 = resize(im_1)\n",
        "        im_2 = resize(im_2)\n",
        "        valid_mask = target[:,:,2:3]\n",
        "        target = resample_flow(target, self.size)\n",
        "        valid_mask = resample_mask(valid_mask, self.size)\n",
        "\n",
        "        i, j, h, w = transforms.RandomCrop.get_params(im_1, output_size=self.crop)\n",
        "        # i, j, h, w = 0, 0, self.crop[0], self.crop[1]\n",
        "        im_1 = TF.crop(im_1, i, j, h, w)\n",
        "        im_2 = TF.crop(im_2, i, j, h, w)\n",
        "        target = target[i:i+h,j:j+w,:]\n",
        "        valid_mask = valid_mask[i:i+h,j:j+w,:]\n",
        "\n",
        "        if random.random() > self.flip:\n",
        "            im_1 = TF.hflip(im_1)\n",
        "            im_2 = TF.hflip(im_2)\n",
        "            target = cv2.flip(target, 1)\n",
        "            valid_mask = cv2.flip(valid_mask, 1)\n",
        "            valid_mask = np.expand_dims(valid_mask, axis=2)\n",
        "            target[:,:,0] = -target[:,:,0] # negate u when flipped\n",
        "\n",
        "        target = target.transpose((2, 0, 1))\n",
        "        valid_mask = valid_mask.transpose((2, 0, 1))\n",
        "\n",
        "        im_1 = TF.to_tensor(im_1)\n",
        "        im_2 = TF.to_tensor(im_2)\n",
        "        target = torch.Tensor(target)\n",
        "        valid_mask = torch.Tensor(valid_mask)\n",
        "\n",
        "        im_1 = norm(im_1)\n",
        "        im_2 = norm(im_2)\n",
        "\n",
        "        scale_factor = 0.5\n",
        "\n",
        "        if scale_factor != 1:\n",
        "            im_1 = im_1.unsqueeze(0)\n",
        "            im_2 = im_2.unsqueeze(0)\n",
        "            target = target.unsqueeze(0)\n",
        "            valid_mask = valid_mask.unsqueeze(0)\n",
        "\n",
        "            im_1 = F.interpolate(im_1, scale_factor=scale_factor, mode='bilinear', align_corners=False)\n",
        "            im_2 = F.interpolate(im_2, scale_factor=scale_factor, mode='bilinear', align_corners=False)\n",
        "            target = F.interpolate(target, scale_factor=scale_factor, mode='bilinear', align_corners=False) * scale_factor\n",
        "            valid_mask = F.interpolate(valid_mask, scale_factor=scale_factor, mode='bilinear', align_corners=False)\n",
        "            \n",
        "            im_1 = im_1.squeeze(0)\n",
        "            im_2 = im_2.squeeze(0)\n",
        "            target = target.squeeze(0)\n",
        "            valid_mask = valid_mask.squeeze(0)\n",
        "\n",
        "        valid_idx = (valid_mask != 0)\n",
        "        valid_mask[valid_idx] = 1\n",
        "\n",
        "        return im_1, im_2, target, valid_mask\n",
        "\n",
        "    def denorm(self, im):\n",
        "        return self.inv_norm(im)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHeP6eO8bXYF",
        "colab_type": "text"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCiDE0uES_IA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class KITTIFlowDataset(Dataset):\n",
        "    def __init__(self, root, tfm_function, image_folder=\"image_2\", flow_folder=\"flow_occ\"):\n",
        "        super(KITTIFlowDataset, self).__init__()\n",
        "        self.root = Path(root)\n",
        "        self.image_dir = self.root/image_folder\n",
        "        self.flow_dir = self.root/flow_folder\n",
        "        self.im_1 = sorted([os.path.basename(x) for x in self.image_dir.glob(\"*_10.png\")])\n",
        "        self.im_2 = sorted([os.path.basename(x) for x in self.image_dir.glob(\"*_11.png\")])\n",
        "        self.tfm_function = tfm_function\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.im_1)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        im_1 = Image.open(self.image_dir/self.im_1[idx]).convert('RGB')\n",
        "        im_2 = Image.open(self.image_dir/self.im_2[idx]).convert('RGB')\n",
        "        target = read_png_flow(self.flow_dir/self.im_1[idx])\n",
        "        \n",
        "        im_1, im_2, target, valid_mask = self.tfm_function(im_1, im_2, target)\n",
        "\n",
        "        return im_1, im_2, target, valid_mask # The third channel of target is all zeros. I think the first two channels are the x and y components of the vector for that pixel "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wy_OHnW-Tgvh",
        "colab_type": "code",
        "outputId": "8808ff8f-f7e7-4d49-a000-27e1b08b3f69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "mean = np.array([.5, .5, .5])\n",
        "std = np.array([.5, .5, .5])\n",
        "norm = transforms.Normalize(mean=mean, std=std)\n",
        "tfms = FlowDatasetTransform(norm)\n",
        "dataset = KITTIFlowDataset(\"data_scene_flow/training\", tfms)\n",
        "len(dataset)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPB9-iF7Op1j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainset = Subset(dataset, np.arange(100))\n",
        "valset = Subset(dataset, np.arange(101, 200))\n",
        "sampler = RandomSampler(trainset)\n",
        "trainloader = DataLoader(trainset, sampler=sampler,  num_workers=4, pin_memory=True, batch_size=4)#, collate_fn=H5MiniBatchDataset.collate, batch_size=2)\n",
        "valloader = DataLoader(valset, batch_size=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFIBmCkboIDg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "backwarp_tenGrid = {}\n",
        "backwarp_tenPartial = {}\n",
        "\n",
        "def warp(im, flow, device='cuda'):\n",
        "    if str(flow.size()) not in backwarp_tenGrid:\n",
        "        tenHorizontal = torch.linspace(-1.0, 1.0, flow.shape[3]).view(1, 1, 1, flow.shape[3]).expand(flow.shape[0], -1, flow.shape[2], -1)\n",
        "        tenVertical = torch.linspace(-1.0, 1.0, flow.shape[2]).view(1, 1, flow.shape[2], 1).expand(flow.shape[0], -1, -1, flow.shape[3])\n",
        "        backwarp_tenGrid[str(flow.size())] = torch.cat([ tenHorizontal, tenVertical ], 1).to(device)\n",
        "\n",
        "    if str(flow.size()) not in backwarp_tenPartial:\n",
        "        backwarp_tenPartial[str(flow.size())] = flow.new_ones([ flow.shape[0], 1, flow.shape[2], flow.shape[3] ])\n",
        "\n",
        "    flow = torch.cat([ flow[:, 0:1, :, :] / ((im.shape[3] - 1.0) / 2.0), flow[:, 1:2, :, :] / ((im.shape[2] - 1.0) / 2.0) ], 1)\n",
        "    im = torch.cat([ im, backwarp_tenPartial[str(flow.size())] ], 1)\n",
        "\n",
        "    grid = (backwarp_tenGrid[str(flow.size())] + flow).permute(0, 2, 3, 1)\n",
        "\n",
        "    out = F.grid_sample(input=im, grid=grid, mode='bilinear', padding_mode='zeros', align_corners=False)\n",
        "\n",
        "    mask = out[:, -1:, :, :]\n",
        "    mask[mask > 0.999] = 1.0\n",
        "    mask[mask < 1.0] = 0.0\n",
        "\n",
        "    return (out[:, :-1, :, :] * mask).contiguous()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRQqyMBKZBlQ",
        "colab_type": "text"
      },
      "source": [
        "# Trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dz_660LZ2395",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0ta5aW-OcD9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PyramidLevel(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels):\n",
        "        super(PyramidLevel, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(nn.Conv2d(in_channels=in_channels, out_channels=hidden_channels, kernel_size=3, stride=2, padding=1),\n",
        "                                   nn.LeakyReLU(inplace=False, negative_slope=0.1),\n",
        "                                   nn.Conv2d(in_channels=hidden_channels, out_channels=hidden_channels, kernel_size=3, stride=1, padding=1),\n",
        "                                   nn.LeakyReLU(inplace=False, negative_slope=0.1),\n",
        "                                   nn.Conv2d(in_channels=hidden_channels, out_channels=hidden_channels, kernel_size=3, stride=1, padding=1),\n",
        "                                   nn.LeakyReLU(inplace=False, negative_slope=0.1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "class FeaturePyramid(nn.Module):\n",
        "  def __init__(self, in_channels=3, hidden_channels=[16, 32, 64, 96, 128, 196]):\n",
        "    super(FeaturePyramid, self).__init__()\n",
        "\n",
        "    self.l1 = PyramidLevel(3, hidden_channels[0])\n",
        "    self.levels = nn.ModuleList([PyramidLevel(hidden_channels[idx], c) for idx, c in enumerate(hidden_channels[1:])])\n",
        "      \n",
        "  def forward(self, x):\n",
        "    features = [self.l1(x)]\n",
        "\n",
        "    for lvl in self.levels:\n",
        "      features.append(lvl(features[-1]))\n",
        "\n",
        "    return features\n",
        "\n",
        "class DenseFlowEstimator(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(DenseFlowEstimator, self).__init__()\n",
        "        self.netOne = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_channels, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.LeakyReLU(inplace=False, negative_slope=0.1)\n",
        "        )\n",
        "\n",
        "        self.netTwo = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_channels + 128, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.LeakyReLU(inplace=False, negative_slope=0.1)\n",
        "        )\n",
        "\n",
        "        self.netThr = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_channels + 128 + 128, out_channels=96, kernel_size=3, stride=1, padding=1),\n",
        "            nn.LeakyReLU(inplace=False, negative_slope=0.1)\n",
        "        )\n",
        "\n",
        "        self.netFou = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_channels + 128 + 128 + 96, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.LeakyReLU(inplace=False, negative_slope=0.1)\n",
        "        )\n",
        "\n",
        "        self.netFiv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_channels + 128 + 128 + 96 + 64, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.LeakyReLU(inplace=False, negative_slope=0.1)\n",
        "        )\n",
        "\n",
        "        self.predict_flow = nn.Conv2d(in_channels=in_channels + 128 + 128 + 96 + 64 + 32, out_channels=2, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.cat([self.netOne(x), x], 1)\n",
        "        x = torch.cat([self.netTwo(x), x], 1)\n",
        "        x = torch.cat([self.netThr(x), x], 1)\n",
        "        x = torch.cat([self.netFou(x), x], 1)\n",
        "        x = torch.cat([self.netFiv(x), x], 1)\n",
        "\n",
        "        flow = self.predict_flow(x)\n",
        "\n",
        "        return x, flow\n",
        "\n",
        "class FlowEstimator(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(FlowEstimator, self).__init__()\n",
        "        self.netOne = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_channels, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.LeakyReLU(inplace=False, negative_slope=0.1)\n",
        "        )\n",
        "\n",
        "        self.netTwo = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.LeakyReLU(inplace=False, negative_slope=0.1)\n",
        "        )\n",
        "\n",
        "        self.netThr = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=128, out_channels=96, kernel_size=3, stride=1, padding=1),\n",
        "            nn.LeakyReLU(inplace=False, negative_slope=0.1)\n",
        "        )\n",
        "\n",
        "        self.netFou = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=96, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.LeakyReLU(inplace=False, negative_slope=0.1)\n",
        "        )\n",
        "\n",
        "        self.netFiv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=64, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.LeakyReLU(inplace=False, negative_slope=0.1)\n",
        "        )\n",
        "\n",
        "        self.predict_flow = nn.Conv2d(in_channels=32, out_channels=2, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.netOne(x)\n",
        "        x = self.netTwo(x)\n",
        "        x = self.netThr(x)\n",
        "        x = self.netFou(x)\n",
        "        x = self.netFiv(x)\n",
        "\n",
        "        flow = self.predict_flow(x)\n",
        "\n",
        "        return x, flow\n",
        "\n",
        "class WarpAndUpsample(nn.Module):\n",
        "  def __init__(self, in_channels, corr, flow_scale_factor=1):\n",
        "    super(WarpAndUpsample, self).__init__()\n",
        "    self.corr = corr\n",
        "    self.upflow = nn.ConvTranspose2d(in_channels=2, out_channels=2, kernel_size=4, stride=2, padding=1)\n",
        "    self.upfeat = nn.ConvTranspose2d(in_channels=in_channels, out_channels=2, kernel_size=4, stride=2, padding=1)\n",
        "    self.flow_scale_factor = flow_scale_factor\n",
        "\n",
        "  def forward(self, feat, im1, im2, flow, gt_flow=None, weights=[1, 0]):\n",
        "    upflow = self.upflow(flow)\n",
        "    warpflow = upflow  \n",
        "\n",
        "    if gt_flow is not None and weights[1] != 0:\n",
        "      gt_flow = F.interpolate(gt_flow, size=(upflow.shape[2], upflow.shape[3]), mode='bilinear', align_corners=False) * 0.05\n",
        "      warpflow = (upflow * weights[0] + gt_flow * weights[1]) / (weights[0] + weights[1])\n",
        "\n",
        "    im2_warp = warp(im2, warpflow*self.flow_scale_factor)\n",
        "    corr = F.leaky_relu(self.corr(im1, im2_warp), negative_slope=0.1)\n",
        "    feat = self.upfeat(feat) \n",
        "    feat = torch.cat([feat, im1, corr, upflow], dim=1)\n",
        "    return feat\n",
        "\n",
        "class ContextNetwork(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(ContextNetwork, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_channels, out_channels=128, kernel_size=3, stride=1, padding=1, dilation=1),\n",
        "            nn.LeakyReLU(inplace=False, negative_slope=0.1),\n",
        "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=2, dilation=2),\n",
        "            nn.LeakyReLU(inplace=False, negative_slope=0.1),\n",
        "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=4, dilation=4),\n",
        "            nn.LeakyReLU(inplace=False, negative_slope=0.1),\n",
        "            nn.Conv2d(in_channels=128, out_channels=96, kernel_size=3, stride=1, padding=8, dilation=8),\n",
        "            nn.LeakyReLU(inplace=False, negative_slope=0.1),\n",
        "            nn.Conv2d(in_channels=96, out_channels=64, kernel_size=3, stride=1, padding=16, dilation=16),\n",
        "            nn.LeakyReLU(inplace=False, negative_slope=0.1),\n",
        "            nn.Conv2d(in_channels=64, out_channels=32, kernel_size=3, stride=1, padding=1, dilation=1),\n",
        "            nn.LeakyReLU(inplace=False, negative_slope=0.1),\n",
        "            nn.Conv2d(in_channels=32, out_channels=2, kernel_size=3, stride=1, padding=1, dilation=1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
        "        nn.init.kaiming_normal_(m.weight.data, mode='fan_in')\n",
        "        if m.bias is not None:\n",
        "            m.bias.data.zero_()\n",
        "\n",
        "class PWCNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(PWCNet, self).__init__()\n",
        "\n",
        "    pyr_channels = [16, 32, 64, 96, 128]\n",
        "\n",
        "    self.feature_pyramid = FeaturePyramid(3, pyr_channels)\n",
        "\n",
        "    md = 4\n",
        "    nd = (2*md+1)**2\n",
        "\n",
        "    self.corr = Correlation(pad_size=md, kernel_size=1, max_displacement=md, stride1=1, stride2=1, corr_multiply=1)\n",
        "\n",
        "    #self.flow_estimator_6 = FlowEstimator(nd)\n",
        "    #self.warp6 = WarpAndUpsample(32, self.corr, 0.0625)\n",
        "\n",
        "    self.flow_estimator_5 = FlowEstimator(nd)\n",
        "    self.warp5 = WarpAndUpsample(32, self.corr, 1.25)\n",
        "\n",
        "    self.flow_estimator_4 = FlowEstimator(nd+2+2+pyr_channels[-2])\n",
        "    self.warp4 = WarpAndUpsample(32, self.corr, 2.5)\n",
        "\n",
        "    self.flow_estimator_3 = FlowEstimator(nd+2+2+pyr_channels[-3])\n",
        "    self.warp3 = WarpAndUpsample(32, self.corr, 5)\n",
        "\n",
        "    self.flow_estimator_2 = FlowEstimator(nd+2+2+pyr_channels[-4])\n",
        "    self.context_net = ContextNetwork(32+2)\n",
        "\n",
        "    self.apply(init_weights)\n",
        "\n",
        "\n",
        "  def forward(self, im1, im2, gt_flow=None):\n",
        "    #l1_1, l2_1, l3_1, l4_1, l5_1, l6_1 = self.feature_pyramid(im1)\n",
        "    #l1_2, l2_2, l3_2, l4_2, l5_2, l6_2 = self.feature_pyramid(im2)\n",
        "\n",
        "    l1_1, l2_1, l3_1, l4_1, l5_1 = self.feature_pyramid(im1)\n",
        "    l1_2, l2_2, l3_2, l4_2, l5_2 = self.feature_pyramid(im2)\n",
        "\n",
        "    #feat = F.leaky_relu(self.corr(l6_1, l6_2), negative_slope=0.1)\n",
        "    feat = F.leaky_relu(self.corr(l5_1, l5_2), negative_slope=0.1)\n",
        "\n",
        "    #feat, flow6 = self.flow_estimator_6(feat)  \n",
        "    #feat = self.warp6(feat, l5_1, l5_2, flow6)\n",
        "\n",
        "    feat, flow5 = self.flow_estimator_5(feat) \n",
        "    feat = self.warp5(feat, l4_1, l4_2, flow5, gt_flow)\n",
        "\n",
        "    feat, flow4 = self.flow_estimator_4(feat)    \n",
        "    feat = self.warp4(feat, l3_1, l3_2, flow4, gt_flow)\n",
        "\n",
        "    feat, flow3 = self.flow_estimator_3(feat)\n",
        "    feat = self.warp3(feat, l2_1, l2_2, flow3, gt_flow)\n",
        "\n",
        "    feat, flow2 = self.flow_estimator_2(feat)\n",
        "\n",
        "    feat = torch.cat([feat, flow2], dim=1)\n",
        "    flow2 = flow2 + self.context_net(feat)  \n",
        "\n",
        "    if self.training:\n",
        "      return [flow2,flow3,flow4,flow5]\n",
        "      # return [flow2,flow3,flow4,flow5,flow6]\n",
        "        \n",
        "    else:\n",
        "      return [flow2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ez7EKDq-k154",
        "colab_type": "text"
      },
      "source": [
        "## Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nymkk1nZmhaQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0spP8kgnGPo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def L2(output, target):\n",
        "    return torch.norm(output-target,p=2,dim=1).mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYNOp03dk3IZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PWCNetLoss(nn.Module):\n",
        "    def __init__(self, alphas=[0.32, 0.08, 0.02, 0.01, 0.005]):\n",
        "        super(PWCNetLoss, self).__init__()\n",
        "        self.alphas = alphas\n",
        "\n",
        "    def forward(self, output, target):\n",
        "        loss = 0\n",
        "        target = target * 0.05 # From the paper: \"We scale the ground truth flow by 20...\" this is a bit confusing.\n",
        "\n",
        "        for pred_flow, alpha in zip(output, self.alphas):\n",
        "            scaled_target = F.interpolate(target, size=(pred_flow.shape[2], pred_flow.shape[3]), mode='bilinear', align_corners=False)\n",
        "            loss += alpha * L2(pred_flow, scaled_target)\n",
        "\n",
        "        return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKRQVkSXLi4g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def L1(output, target, q, epsilon, mask):\n",
        "    # print(\"output: \",output.shape)\n",
        "    # print(\"target: \",target.shape)\n",
        "    # print(\"mask: \",target.shape)\n",
        "    # epsilon = 0\n",
        "    return (mask*(torch.abs(output-target)+epsilon)**q).mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XqoQ5dVJstEG",
        "colab": {}
      },
      "source": [
        "class PWCNetLoss_Kitti_finetune(nn.Module):\n",
        "    def __init__(self, alphas=[0.32, 0.08, 0.02, 0.01, 0.005],q=0.4,epsilon=0.01):\n",
        "        super(PWCNetLoss_Kitti_finetune, self).__init__()\n",
        "        self.alphas = alphas\n",
        "        self.q = q\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def forward(self, output, target, mask):\n",
        "        loss = 0\n",
        "        target = target * 0.05 # From the paper: \"We scale the ground truth flow by 20...\" this is a bit confusing.\n",
        "\n",
        "        #exclued invalid pixels\n",
        "\n",
        "        for pred_flow, alpha in zip(output, self.alphas):\n",
        "            scaled_target = F.interpolate(target, size=(pred_flow.shape[2], pred_flow.shape[3]), mode='bilinear', align_corners=False)\n",
        "            scaled_mask = F.interpolate(mask, size=(pred_flow.shape[2], pred_flow.shape[3]), mode='bilinear', align_corners=False)\n",
        "            valid_idx = (scaled_mask != 0)\n",
        "            scaled_mask[valid_idx] = 1\n",
        "            loss += alpha * L1(pred_flow, scaled_target, self.q, self.epsilon, scaled_mask)\n",
        "\n",
        "        return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjPmzvzR4nl5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_custom(model, optimizer, objective, schedule):\n",
        "    cb_dict = {}\n",
        "\n",
        "    with TrainModel(model):\n",
        "        schedule.on_train_begin(model, optimizer)\n",
        "\n",
        "        for epoch in schedule:\n",
        "\n",
        "            schedule.on_epoch_begin(model, optimizer)\n",
        "\n",
        "            for im1, im2, target in schedule.data():\n",
        "                schedule.on_batch_begin(model, optimizer)\n",
        "\n",
        "                im1, im2, target = Variable(im1.to('cuda')), Variable(im2.to('cuda')), Variable(target.to('cuda'))\n",
        "\n",
        "                out = model(im1, im2, target)\n",
        "                loss = objective(out, target)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                schedule.on_batch_end(model, optimizer, loss.detach().item())\n",
        "\n",
        "                del loss, im1, im2, target\n",
        "\n",
        "            schedule.on_epoch_end(model, optimizer)\n",
        "\n",
        "        schedule.on_train_end(model, optimizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcN6X4Z7Z8g0",
        "colab_type": "text"
      },
      "source": [
        "## Load Model and Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrfqdnkiemV5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from baseline import *\n",
        "\n",
        "img_root = '/content/drive/Shared drives/EECS 504 PWC Net/Test Images'\n",
        "save_root = '/content/drive/Shared drives/EECS 504 PWC Net/Results/Baseline_Results/Dense_GF/'\n",
        "img1_name = 'car1.jpg'\n",
        "img2_name = 'car2.jpg'\n",
        "save_name = 'car.png'\n",
        "\n",
        "os.chdir(img_root)\n",
        "img1 = cv2.imread(img1_name)\n",
        "img2 = cv2.imread(img2_name)\n",
        "\n",
        "params = dict(pyr_scale = 0.5,\n",
        "                levels = 5,\n",
        "                winsize = 15, \n",
        "                iterations = 5, \n",
        "                poly_n = 7, \n",
        "                poly_sigma = 1.5,\n",
        "                flags = 0)\n",
        "    \n",
        "flow = dense_GF(img1, img2, params, True)\n",
        "flow_color = flow_vis.flow_to_color(flow[:,:,0:2])\n",
        "\n",
        "plot_flow(img1, img2, flow_color)\n",
        "# write_png_flow(flow, save_root+save_name)\n",
        "# cv2.imwrite(save_root+'color_'+save_name, cv2.cvtColor(flow_color, cv2.COLOR_BGR2RGB))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDvjObwvzB4b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# root = '/content/drive/Shared drives/EECS 504 PWC Net/Models/0419-1109PM_Test-Kitti-model.ckpt.tar'\n",
        "root = '/content/drive/Shared drives/EECS 504 PWC Net/Models/Test-Kitti-model.ckpt.tar'\n",
        "checkpoint = torch.load(root, map_location=None)\n",
        "model = PWCNet().to('cuda')\n",
        "model.load_state_dict(checkpoint['model'])\n",
        "model.eval();"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qr4rX6d82kgv",
        "colab_type": "code",
        "outputId": "55f65f35-b3d2-478c-9c0f-4dd7331a1040",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import datetime\n",
        "\n",
        "x = datetime.datetime.now()\n",
        "date = x.strftime(\"%Y\")+x.strftime(\"%m\")+x.strftime(\"%d\")\n",
        "root_date = '/content/drive/Shared drives/EECS 504 PWC Net/Results/PWC-Net/'+date+'/'\n",
        "\n",
        "if os.path.exists(root_date):\n",
        "    print('Path Exists')\n",
        "    time = x.strftime(\"%H\")+x.strftime(\"%M\")\n",
        "    root_date = root_date + date + time\n",
        "\n",
        "root_image_2 = root_date+'/image_2/'\n",
        "root_image_3 = root_date+'/image_3/'\n",
        "\n",
        "os.makedirs(root_image_2, exist_ok=True)\n",
        "os.makedirs(root_image_3, exist_ok=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Path Exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9HNS7DQ_pq1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('/content/')\n",
        "mean = np.array([.5, .5, .5])\n",
        "std = np.array([.5, .5, .5])\n",
        "norm = transforms.Normalize(mean=mean, std=std)\n",
        "tfms = FlowDatasetTransform(norm, flip=1)\n",
        "dataset = KITTIFlowDataset('/content/drive/Shared drives/EECS 504 PWC Net/Data/data_scene_flow/training', tfms)\n",
        "\n",
        "trainset = Subset(dataset, np.arange(100))\n",
        "valset = Subset(dataset, np.arange(100, 200))\n",
        "sampler = RandomSampler(trainset)\n",
        "trainloader = DataLoader(trainset, sampler=sampler,  num_workers=4, pin_memory=True, batch_size=4)#, collate_fn=H5MiniBatchDataset.collate, batch_size=2)\n",
        "valloader = DataLoader(valset, batch_size=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CmGfSI-Dxfv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from evaluation import *\n",
        "import time\n",
        "\n",
        "# img_root = '/content/drive/Shared drives/EECS 504 PWC Net/Data/data_scene_flow/training/image_2/'\n",
        "# imgs_1 = sorted([os.path.basename(x) for x in image_dir.glob('*_10.png')])\n",
        "# imgs_2 = sorted([os.path.basename(x) for x in image_dir.glob('*_11.png')])\n",
        "\n",
        "save_root = root_image_2\n",
        "GT_root = '/content/drive/Shared drives/EECS 504 PWC Net/Data/data_scene_flow/training/flow_occ/'\n",
        "save_name = 0\n",
        "\n",
        "# FL_up_est = np.asarray([])\n",
        "# AEPE_up_est = np.asarray([])\n",
        "\n",
        "FL_down_gt_pwc = np.asarray([])\n",
        "AEPE_down_gt_pwc = np.asarray([])\n",
        "FL_down_gt_GF = np.asarray([])\n",
        "AEPE_down_gt_GF = np.asarray([])\n",
        "FL_down_gt_LK = np.asarray([])\n",
        "AEPE_down_gt_LK = np.asarray([])\n",
        "\n",
        "time_list_pwc = []\n",
        "time_list_GF = []\n",
        "time_list_LK = []\n",
        "\n",
        "params_GF = dict(pyr_scale = 0.5,\n",
        "                levels = 5,\n",
        "                winsize = 15, \n",
        "                iterations = 5, \n",
        "                poly_n = 7, \n",
        "                poly_sigma = 1.5,\n",
        "                flags = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhLKiNKc2Aky",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "from baseline import *\n",
        "from evaluation import *\n",
        "count = 100\n",
        "tau = np.asarray([5.0, 0.5])\n",
        "for img1, img2, target, mask in valloader:\n",
        "    # img1, img2, target, mask = next(iter(valloader)) \n",
        "    im1, im2 = Variable(img1.to('cuda')), Variable(img2.to('cuda'))\n",
        "\n",
        "    ## PWC-NET Esimation\n",
        "    flow_target = np.moveaxis(target.squeeze().numpy(), 0, -1)\n",
        "\n",
        "    t = time.time()\n",
        "    flow_pwc = model(im1, im2)\n",
        "    elapsed_pwc = time.time() - t\n",
        "    time_list_pwc.append(elapsed_pwc)\n",
        "\n",
        "    flow_pwc = flow_pwc[0].squeeze().to('cpu').detach().numpy()\n",
        "    flow_pwc = np.moveaxis(flow_pwc, 0, -1)\n",
        "\n",
        "    img1 = img1.squeeze()\n",
        "    img2 = img2.squeeze()\n",
        "    img1 = tfms.denorm(img1)*255\n",
        "    img2 = tfms.denorm(img2)*255\n",
        "    img1 = np.moveaxis(img1.numpy(), 0, -1)\n",
        "    img2 = np.moveaxis(img2.numpy(), 0, -1)\n",
        "    \n",
        "    ## Dense GF Estimation    \n",
        "    prev_img = cv2.cvtColor(img1,cv2.COLOR_RGB2GRAY)\n",
        "    next_img = cv2.cvtColor(img2,cv2.COLOR_RGB2GRAY)\n",
        "    t = time.time()\n",
        "    flow_GF = cv2.calcOpticalFlowFarneback(prev_img, next_img, None, **params_GF)\n",
        "    elapsed_GF = time.time() - t\n",
        "    time_list_GF.append(elapsed_GF)\n",
        "\n",
        "    ## Dense LK Estimation\n",
        "    t = time.time()\n",
        "    flow_LK = dense_LK(img1, img2)\n",
        "    elapsed_LK = time.time() - t\n",
        "    time_list_LK.append(elapsed_LK)\n",
        "\n",
        "    ## Evaluation   \n",
        "\n",
        "    flow_mask = mask.view(mask.shape[2], mask.shape[3], -1).numpy()\n",
        "    F_gt = np.concatenate((flow_target, flow_mask), axis=2)\n",
        "\n",
        "    # fl_up_est, aepe_up_est = flow_error(F_gt, cv2.resize(flow, (F_gt.shape[1], F_gt.shape[0])))\n",
        "    fl_down_gt_pwc, aepe_down_gt_pwc = flow_error(cv2.resize(F_gt, (flow_pwc.shape[1], flow_pwc.shape[0])), flow_pwc, tau)\n",
        "    fl_down_gt_GF, aepe_down_gt_GF = flow_error(F_gt, flow_GF, tau)\n",
        "    fl_down_gt_LK, aepe_down_gt_LK = flow_error(F_gt, flow_LK, tau)\n",
        "\n",
        "    # FL_up_est = np.append(FL_up_est, fl_up_est)\n",
        "    # AEPE_up_est = np.append(AEPE_up_est, aepe_up_est)\n",
        "\n",
        "    FL_down_gt_pwc = np.append(FL_down_gt_pwc, fl_down_gt_pwc)\n",
        "    AEPE_down_gt_pwc = np.append(AEPE_down_gt_pwc, aepe_down_gt_pwc)\n",
        "    FL_down_gt_GF = np.append(FL_down_gt_GF, fl_down_gt_GF)\n",
        "    AEPE_down_gt_GF = np.append(AEPE_down_gt_GF, aepe_down_gt_GF)\n",
        "    FL_down_gt_LK = np.append(FL_down_gt_LK, fl_down_gt_LK)\n",
        "    AEPE_down_gt_LK = np.append(AEPE_down_gt_LK, aepe_down_gt_LK)\n",
        "\n",
        "    # ## Plot\n",
        "    # print(count)\n",
        "    \n",
        "    # # flow_target_color = flow_vis.flow_to_color(flow_target, convert_to_bgr=False)\n",
        "    # flow_target_color = flow_vis.flow_to_color(F_gt[:,:,0:2], convert_to_bgr=False)\n",
        "\n",
        "    # flow_pwc_color  = flow_vis.flow_to_color(flow_pwc)\n",
        "    # flow_GF_color  = flow_vis.flow_to_color(flow_GF)\n",
        "    # flow_LK_color  = flow_vis.flow_to_color(flow_LK)\n",
        "\n",
        "    # # titles = ['Prev Image', 'Next Image', 'Est Flow', 'GT Flow']\n",
        "    # fig, ax = plt.subplots(2, 3, figsize=(30, 5))\n",
        "    \n",
        "    # # fig, ax = plt.subplots(221)\n",
        "    # # fig.suptitle(img1_name, y=0.62)\n",
        "    # for i, a in enumerate(ax.flatten()):\n",
        "    #   a.set_axis_off()\n",
        "    # #   a.set_title(titles[i])\n",
        "    # ax[0, 0].imshow(img1/255)\n",
        "    # ax[0, 1].imshow(img2/255)\n",
        "    # ax[0, 2].imshow(flow_target_color)\n",
        "    # ax[1, 0].imshow(flow_GF_color)\n",
        "    # ax[1, 1].imshow(flow_LK_color)\n",
        "    # ax[1, 2].imshow(flow_pwc_color)\n",
        "    \n",
        "    # # write_png_flow(flow, save_root+str(save_name)+'.png');\n",
        "    # cv2.imwrite(save_root+str(count)+'_img1.png', cv2.cvtColor(img1, cv2.COLOR_BGR2RGB))\n",
        "    # cv2.imwrite(save_root+str(count)+'_img2.png', cv2.cvtColor(img2, cv2.COLOR_BGR2RGB))\n",
        "    # cv2.imwrite(save_root+str(count)+'_target.png', cv2.cvtColor(flow_target_color, cv2.COLOR_BGR2RGB))\n",
        "    # cv2.imwrite(save_root+str(count)+'_GF.png', cv2.cvtColor(flow_GF_color, cv2.COLOR_BGR2RGB))\n",
        "    # cv2.imwrite(save_root+str(count)+'_LK.png', cv2.cvtColor(flow_LK_color, cv2.COLOR_BGR2RGB))\n",
        "    # cv2.imwrite(save_root+str(count)+'_pwc.png', cv2.cvtColor(flow_pwc_color, cv2.COLOR_BGR2RGB))\n",
        "    # count = count+1\n",
        "    # plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orAJPFamzHah",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def average_error(FL_down_gt, AEPE_down_gt, time_list):\n",
        "    # FL_avg_up_est = np.mean(FL_up_est)\n",
        "    # AEPE_avg_up_est = np.mean(AEPE_up_est)\n",
        "\n",
        "    FL_avg_down_gt = np.mean(FL_down_gt)\n",
        "    AEPE_avg_down_gt = np.mean(AEPE_down_gt)\n",
        "\n",
        "    t_mean = np.mean(np.asarray(time_list))\n",
        "\n",
        "    print(\"Mean running time\", t_mean)\n",
        "    # print('Fl-All Average (Upsample Estimation):', FL_avg_up_est)\n",
        "    # print('AEPE Average(Upsample Estimation): ', AEPE_avg_up_est)\n",
        "    print('Fl-All Average (Downsample GT):', FL_avg_down_gt)\n",
        "    print('AEPE Average(Downsample GT): ', AEPE_avg_down_gt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2t8uQZyWtQL",
        "colab_type": "code",
        "outputId": "c0e65849-8ec7-44a0-ef9b-f17686fae01d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "print(\"PWC-Net\")\n",
        "average_error(FL_down_gt_pwc, AEPE_down_gt_pwc, time_list_pwc)\n",
        "print(\"\\nGF\")\n",
        "average_error(FL_down_gt_GF, AEPE_down_gt_GF, time_list_GF)\n",
        "print(\"\\nLK\")\n",
        "average_error(FL_down_gt_LK, AEPE_down_gt_LK, time_list_LK)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PWC-Net\n",
            "Mean running time 0.010814585685729981\n",
            "Fl-All Average (Downsample GT): 0.449372474641487\n",
            "AEPE Average(Downsample GT):  5.817433965334745\n",
            "\n",
            "GF\n",
            "Mean running time 0.030926355838775635\n",
            "Fl-All Average (Downsample GT): 0.4723166590624772\n",
            "AEPE Average(Downsample GT):  6.667518096961297\n",
            "\n",
            "LK\n",
            "Mean running time 2.182913969039917\n",
            "Fl-All Average (Downsample GT): 0.42085547587860095\n",
            "AEPE Average(Downsample GT):  5.981375852854\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}